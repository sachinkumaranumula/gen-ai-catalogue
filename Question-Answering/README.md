# Architecture
![RAG Architecture](https://docs.llamaindex.ai/en/stable/_static/getting_started/basic_rag.png)

# Process
![Process](https://docs.llamaindex.ai/en/stable/_static/getting_started/stages.pngp)

# Variants
Most common use-cases for LLMs is to answer questions over a set of data. The predominant framework for enabling QA with LLMs is Retrieval Augmented Generation (RAG). The simplest queries involve either 
    - *Semantic Search*: A query about specific information in a document that matches the query terms and/or semantic intent. This is typically executed with simple vector retrieval (top-k)
    - *Summarization*: Condensing a large amount of data into a short summary relevant to your current question.

# Approaches
- Querying Complex Documents
- Combine multiple sources
- Route across multiple sources
- Multi-document queries

# Framework
## LlamaIndex
 - https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/
 - https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/q_and_a/
